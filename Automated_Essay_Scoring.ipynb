{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automated Essay Scoring.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ348PaMFqwN"
      },
      "source": [
        "**Here I use Word embeddings method to create the word vectors. Word2Vec is a better alternative to the given problem but I manually wanted to check the entire process to avoid certain logical errors from my side.\n",
        "Since it was mentioned in the word document that \"*The current problem for this assignment only looks at evaluating the essays based on Content; please feel free to ignore the Grammar and Flow modelling for now.**\", **I resorted to the use of LSTM(unidirection) to make the process faster. ****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxdQKyrYFqwR"
      },
      "source": [
        "**The aim of this model is to enable automatic evaluation of the paragraphs keeping in reference the content of the paragraph.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpS3JlayFqwS"
      },
      "source": [
        "# **Importing the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DD1pvpyFqwS",
        "outputId": "bd1bc100-d592-4c09-97d1-fc98fc384542"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.40.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2BsD9gWtig8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import math\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from keras_tuner.tuners import Hyperband\n",
        "from keras_tuner.engine.hyperparameters import HyperParameters\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryLzt6VPDo9t"
      },
      "source": [
        "# Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a84tkn7NuG9F",
        "outputId": "c04334b6-90ea-4a36-9f8a-42b6d8891317"
      },
      "source": [
        "#Importing the train dataset\n",
        "train_dataset= pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "#importing the test dataset\n",
        "test_dataset= pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "#importing the prompts\n",
        "prompts= pd.read_csv(\"/content/all_prompts.csv\")\n",
        "prompts=prompts.iloc[:, 1:]\n",
        "\n",
        "#seperating the independent values\n",
        "train_dataset=train_dataset.iloc[:, 1:]\n",
        "test_dataset=test_dataset.iloc[:, 1:]\n",
        "\n",
        "#here we store the dependent variable in y and drop the column\n",
        "y=train_dataset.evaluator_rating.values\n",
        "train_dataset=train_dataset.drop(\"evaluator_rating\", axis=1)\n",
        "\n",
        "#number of rows of training dataset\n",
        "num_rows_train= len(y)\n",
        "\n",
        "#number of rows of the test dataset\n",
        "num_rows_test=len(test_dataset.promptId.values)\n",
        "\n",
        "\n",
        "#Here I combine the independent values of the training and test dataset to enable proper Word Embeddings \n",
        "#with same vocabulary. This two datasets will be seperated later\n",
        "frame= [train_dataset, test_dataset]\n",
        "dataset = pd.concat(frame)\n",
        "dataset.reset_index(inplace=True)\n",
        "total_rows= len(dataset.index.values)\n",
        "\n",
        "\n",
        "#next I join the prompt question to the respective essays so \n",
        "#as to preserve the context of both the question and answer\n",
        "column_values=[]\n",
        "keys= prompts.promptId.values\n",
        "values= prompts.prompt_question.values\n",
        "iterator= dataset.promptId.values\n",
        "for i in range(0, total_rows):\n",
        "    dataset[\"essay\"][i]= values[np.where(keys== dataset[\"promptId\"][i])]+\" \"+dataset[\"essay\"][i]\n",
        "    dataset[\"essay\"][i]= dataset[\"essay\"][i][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "MfR75_ZYJF1X",
        "outputId": "e87a5775-383f-481d-8c53-fc2b7e33d885"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>promptId</th>\n",
              "      <th>uniqueId</th>\n",
              "      <th>essay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1_323</td>\n",
              "      <td>At present age, our education system is not go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1_238</td>\n",
              "      <td>I am agree the tightly defined curriculum of o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1_212</td>\n",
              "      <td>I strongly agree with the statement that tight...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1_117</td>\n",
              "      <td>Our education system is nice quitely but i dis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1_229</td>\n",
              "      <td>i am totally agree with the statement that tig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1235</th>\n",
              "      <td>5</td>\n",
              "      <td>5_419</td>\n",
              "      <td>The entire world is in the race of producing a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236</th>\n",
              "      <td>5</td>\n",
              "      <td>5_420</td>\n",
              "      <td>The race in the development of weapons are pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1237</th>\n",
              "      <td>5</td>\n",
              "      <td>5_421</td>\n",
              "      <td>In an era where every second person hopes and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>5</td>\n",
              "      <td>5_422</td>\n",
              "      <td>INTRODUCTION :Since the beginning of the time ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239</th>\n",
              "      <td>5</td>\n",
              "      <td>5_423</td>\n",
              "      <td>\"To conquer a nation, first disarm its citizen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1240 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      promptId uniqueId                                              essay\n",
              "0            1    1_323  At present age, our education system is not go...\n",
              "1            1    1_238  I am agree the tightly defined curriculum of o...\n",
              "2            1    1_212  I strongly agree with the statement that tight...\n",
              "3            1    1_117  Our education system is nice quitely but i dis...\n",
              "4            1    1_229  i am totally agree with the statement that tig...\n",
              "...        ...      ...                                                ...\n",
              "1235         5    5_419  The entire world is in the race of producing a...\n",
              "1236         5    5_420  The race in the development of weapons are pro...\n",
              "1237         5    5_421  In an era where every second person hopes and ...\n",
              "1238         5    5_422  INTRODUCTION :Since the beginning of the time ...\n",
              "1239         5    5_423  \"To conquer a nation, first disarm its citizen...\n",
              "\n",
              "[1240 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "c19fHQHBJf98",
        "outputId": "821957db-54d3-4c47-b046-ddf572b871f4"
      },
      "source": [
        "test_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>promptId</th>\n",
              "      <th>uniqueId</th>\n",
              "      <th>essay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1_315</td>\n",
              "      <td>Curriculum has been adopted in many schools. T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1_214</td>\n",
              "      <td>I strongly agree with the statement ,  The tig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1_196</td>\n",
              "      <td>Imagination and creativity is the most importa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1_178</td>\n",
              "      <td>In our eduction system leaves no room for imag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1_201</td>\n",
              "      <td>I will agree at some what extend, because if w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>5</td>\n",
              "      <td>5_146</td>\n",
              "      <td>Earth is a creation of God and everything that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>5</td>\n",
              "      <td>5_65</td>\n",
              "      <td>production of arms and weapons in this present...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>5</td>\n",
              "      <td>5_151</td>\n",
              "      <td>Race to become more powerful can destroy the e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>5</td>\n",
              "      <td>5_404</td>\n",
              "      <td>In its attempt to harness the power of the ato...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>5</td>\n",
              "      <td>5_360</td>\n",
              "      <td>Racein the production of arms and weapons in t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>305 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     promptId uniqueId                                              essay\n",
              "0           1    1_315  Curriculum has been adopted in many schools. T...\n",
              "1           1    1_214  I strongly agree with the statement ,  The tig...\n",
              "2           1    1_196  Imagination and creativity is the most importa...\n",
              "3           1    1_178  In our eduction system leaves no room for imag...\n",
              "4           1    1_201  I will agree at some what extend, because if w...\n",
              "..        ...      ...                                                ...\n",
              "300         5    5_146  Earth is a creation of God and everything that...\n",
              "301         5     5_65  production of arms and weapons in this present...\n",
              "302         5    5_151  Race to become more powerful can destroy the e...\n",
              "303         5    5_404  In its attempt to harness the power of the ato...\n",
              "304         5    5_360  Racein the production of arms and weapons in t...\n",
              "\n",
              "[305 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "q8KPHneIJkVg",
        "outputId": "1094faf8-0aa8-4f86-d4c4-b291d7003c99"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>promptId</th>\n",
              "      <th>uniqueId</th>\n",
              "      <th>essay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1_323</td>\n",
              "      <td>The tight curriculum of our education system l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1_238</td>\n",
              "      <td>The tight curriculum of our education system l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1_212</td>\n",
              "      <td>The tight curriculum of our education system l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1_117</td>\n",
              "      <td>The tight curriculum of our education system l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1_229</td>\n",
              "      <td>The tight curriculum of our education system l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1540</th>\n",
              "      <td>300</td>\n",
              "      <td>5</td>\n",
              "      <td>5_146</td>\n",
              "      <td>In the nuclear age, the production and develop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1541</th>\n",
              "      <td>301</td>\n",
              "      <td>5</td>\n",
              "      <td>5_65</td>\n",
              "      <td>In the nuclear age, the production and develop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542</th>\n",
              "      <td>302</td>\n",
              "      <td>5</td>\n",
              "      <td>5_151</td>\n",
              "      <td>In the nuclear age, the production and develop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1543</th>\n",
              "      <td>303</td>\n",
              "      <td>5</td>\n",
              "      <td>5_404</td>\n",
              "      <td>In the nuclear age, the production and develop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1544</th>\n",
              "      <td>304</td>\n",
              "      <td>5</td>\n",
              "      <td>5_360</td>\n",
              "      <td>In the nuclear age, the production and develop...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1545 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  ...                                              essay\n",
              "0         0  ...  The tight curriculum of our education system l...\n",
              "1         1  ...  The tight curriculum of our education system l...\n",
              "2         2  ...  The tight curriculum of our education system l...\n",
              "3         3  ...  The tight curriculum of our education system l...\n",
              "4         4  ...  The tight curriculum of our education system l...\n",
              "...     ...  ...                                                ...\n",
              "1540    300  ...  In the nuclear age, the production and develop...\n",
              "1541    301  ...  In the nuclear age, the production and develop...\n",
              "1542    302  ...  In the nuclear age, the production and develop...\n",
              "1543    303  ...  In the nuclear age, the production and develop...\n",
              "1544    304  ...  In the nuclear age, the production and develop...\n",
              "\n",
              "[1545 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbhuewncFqwW"
      },
      "source": [
        "# Cleaning the paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXBX69kkuyNw",
        "outputId": "9bd75cea-6a67-4e57-df08-d2618eee0688"
      },
      "source": [
        "#cleaning the dataset\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def cleaner(dataset):\n",
        "    length= len(dataset.promptId)\n",
        "    corpus=[]\n",
        "    for i in range(0,length):\n",
        "      review = re.sub('[^a-zA-Z]', ' ', dataset[\"essay\"][i])\n",
        "      review = review.lower()\n",
        "      review = review.split()  \n",
        "      review = [lem.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
        "      review = ' '.join(review)\n",
        "      corpus.append(review)\n",
        "    return corpus\n",
        "\n",
        "#obtaining the final corpus\n",
        "corpus= cleaner(dataset=dataset)\n",
        "\n",
        "\n",
        "#creating a vocabulary and calculate the size of the vocabulary\n",
        "def vocabulary_array(corpus):\n",
        "    vocabulary=[]\n",
        "    for i in corpus:\n",
        "      text=i\n",
        "      text = text.split()\n",
        "      for j in text:\n",
        "        if j not in vocabulary:\n",
        "          vocabulary.append(j)\n",
        "    vocabulary.sort()\n",
        "    return vocabulary\n",
        "\n",
        "vocabulary= vocabulary_array(corpus=corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UwFs2qAFqwX"
      },
      "source": [
        "# Creating the word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s79eOPzRFqwX"
      },
      "source": [
        "#making the embedded docs for the data\n",
        "voc_size= len(vocabulary)\n",
        "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
        "sizes=[]\n",
        "for i in onehot_repr:\n",
        "  sizes.append(len(i))\n",
        "sent_length= max(sizes)\n",
        "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
        "\n",
        "#as mentioned earlier the embedding of the training and test data are seperated\n",
        "train_docs=embedded_docs[:num_rows_train]\n",
        "test_docs=embedded_docs[num_rows_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e8f5TRGFqwY"
      },
      "source": [
        "# Using train test split\n",
        "**With this we have two sets of data. One for training and another for validation.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxAQ9ftODKIc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_docs, y, test_size=0.4, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjI7vV5qFqwY"
      },
      "source": [
        "# Building our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za-bOLwzKcGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f831f8-9789-4f93-bd6a-a29dd3ca08fc"
      },
      "source": [
        "#Hyperparameter tuning the model\n",
        "def build_model_1(hp):\n",
        "  model1 = Sequential()\n",
        "  model1.add(Embedding(input_dim= voc_size, output_dim= 100, input_length=sent_length,))\n",
        "  model1.add(LSTM(hp.Int('input_unit',min_value=32,max_value=512,step=32),return_sequences=True,))\n",
        "  for i in range(hp.Int('n_layers', 1, 10)):\n",
        "    model1.add(LSTM(hp.Int(f'lstm_{i}_units',min_value=32,max_value=512,step=32),return_sequences=True))\n",
        "  model1.add(LSTM(hp.Int(f'lstm_last_unit',min_value=32,max_value=512,step=32)))\n",
        "  model1.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
        "  model1.add(Dense(1, activation=hp.Choice('dense_activation',values=['relu', 'sigmoid'],default='relu')))\n",
        "  model1.compile(loss='mean_squared_error', optimizer='adam',metrics = ['mse'])\n",
        "  return model1\n",
        "\n",
        "\n",
        "tuner1 = Hyperband(build_model_1,\n",
        "                     objective='mse',\n",
        "                     max_epochs=5,\n",
        "                     factor=2,\n",
        "                     directory='/content/',\n",
        "                     project_name='automated_essay')\n",
        "\n",
        "tuner1.search(X_train, y_train, epochs=50, validation_data= [X_val, y_val])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9 Complete [00h 01m 59s]\n",
            "mse: 3.9139785766601562\n",
            "\n",
            "Best mse So Far: 1.1089434623718262\n",
            "Total elapsed time: 00h 16m 12s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "713IyKS6FqwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1765ce0-3f65-4153-fe36-da1a5e5c8136"
      },
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_hps1=tuner1.get_best_hyperparameters(num_trials=1)[0]\n",
        "model1 = tuner1.hypermodel.build(best_hps1)\n",
        "history1 = model1.fit(X_train, y_train, epochs=30)\n",
        "val_acc_per_epoch1 = history1.history['mse']\n",
        "best_epoch1 = val_acc_per_epoch1.index(min(val_acc_per_epoch1)) + 1\n",
        "hypermodel1 = tuner1.hypermodel.build(best_hps1)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel1.fit(X_train, y_train, epochs=best_epoch1)\n",
        "hypermodel1.summary()\n",
        "\n",
        "#hypermodel1.save(\"/content/models\")\n",
        "#from tensorflow import keras\n",
        "#hypermodel1 = keras.models.load_model('/content/models')\n",
        "\n",
        "#model evaluation\n",
        "eval_result = hypermodel1.evaluate(X_val, y_val)\n",
        "print(\"[loss, mse]:\", eval_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "24/24 [==============================] - 13s 277ms/step - loss: 2.5169 - mse: 2.5169\n",
            "Epoch 2/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 1.2054 - mse: 1.2054\n",
            "Epoch 3/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.9950 - mse: 0.9950\n",
            "Epoch 4/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.4681 - mse: 0.4681\n",
            "Epoch 5/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.2601 - mse: 0.2601\n",
            "Epoch 6/30\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.1821 - mse: 0.1821\n",
            "Epoch 7/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.1129 - mse: 0.1129\n",
            "Epoch 8/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0848 - mse: 0.0848\n",
            "Epoch 9/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0730 - mse: 0.0730\n",
            "Epoch 10/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0635 - mse: 0.0635\n",
            "Epoch 11/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0655 - mse: 0.0655\n",
            "Epoch 12/30\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.0605 - mse: 0.0605\n",
            "Epoch 13/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0524 - mse: 0.0524\n",
            "Epoch 14/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0570 - mse: 0.0570\n",
            "Epoch 15/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0535 - mse: 0.0535\n",
            "Epoch 16/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0595 - mse: 0.0595\n",
            "Epoch 17/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0485 - mse: 0.0485\n",
            "Epoch 18/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0503 - mse: 0.0503\n",
            "Epoch 19/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 20/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0470 - mse: 0.0470\n",
            "Epoch 21/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0449 - mse: 0.0449\n",
            "Epoch 22/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 23/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 24/30\n",
            "24/24 [==============================] - 6s 263ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 25/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 26/30\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 27/30\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 28/30\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 29/30\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 30/30\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 1/26\n",
            "24/24 [==============================] - 12s 279ms/step - loss: 2.5336 - mse: 2.5336\n",
            "Epoch 2/26\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 1.2284 - mse: 1.2284\n",
            "Epoch 3/26\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.9241 - mse: 0.9241\n",
            "Epoch 4/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.4524 - mse: 0.4524\n",
            "Epoch 5/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.2546 - mse: 0.2546\n",
            "Epoch 6/26\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.1726 - mse: 0.1726\n",
            "Epoch 7/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0956 - mse: 0.0956\n",
            "Epoch 8/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0681 - mse: 0.0681\n",
            "Epoch 9/26\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.0535 - mse: 0.0535\n",
            "Epoch 10/26\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0493 - mse: 0.0493\n",
            "Epoch 11/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 12/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 13/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0352 - mse: 0.0352\n",
            "Epoch 14/26\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.0340 - mse: 0.0340\n",
            "Epoch 15/26\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0336 - mse: 0.0336\n",
            "Epoch 16/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0321 - mse: 0.0321\n",
            "Epoch 17/26\n",
            "24/24 [==============================] - 6s 267ms/step - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 18/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0410 - mse: 0.0410\n",
            "Epoch 19/26\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 20/26\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 21/26\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 22/26\n",
            "24/24 [==============================] - 6s 266ms/step - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 23/26\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 24/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 25/26\n",
            "24/24 [==============================] - 6s 265ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 26/26\n",
            "24/24 [==============================] - 6s 264ms/step - loss: 0.0328 - mse: 0.0328\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 340, 100)          1937700   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 340, 224)          291200    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 340, 128)          180736    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 340, 96)           86400     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 128)               115200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,611,365\n",
            "Trainable params: 2,611,365\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "16/16 [==============================] - 3s 103ms/step - loss: 1.0822 - mse: 1.0822\n",
            "[loss, mse]: [1.0821980237960815, 1.0821980237960815]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIjQKmmiFqwZ"
      },
      "source": [
        "#now we make our predictions on the test dataset\n",
        "pred= hypermodel1.predict(test_docs)\n",
        "predictions=[i[0] for i in pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taezFzL9FqwZ"
      },
      "source": [
        "**To account for checking the Grammer and Flow, Bi-directional LSTM would be preferred as during the developement I tested this.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwzQ-zC5UvRV"
      },
      "source": [
        "prediction=[]\n",
        "for i in predictions:\n",
        "  j=round(i, 1)\n",
        "  prediction.append(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH-eyHY5VIR_"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mHrCFiwFqwa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "cad202c4-07a6-4b8a-b91f-981daa604b36"
      },
      "source": [
        "test_dataset['predicted_score']=prediction\n",
        "test_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>promptId</th>\n",
              "      <th>uniqueId</th>\n",
              "      <th>essay</th>\n",
              "      <th>predicted_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1_315</td>\n",
              "      <td>Curriculum has been adopted in many schools. T...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1_214</td>\n",
              "      <td>I strongly agree with the statement ,  The tig...</td>\n",
              "      <td>3.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1_196</td>\n",
              "      <td>Imagination and creativity is the most importa...</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1_178</td>\n",
              "      <td>In our eduction system leaves no room for imag...</td>\n",
              "      <td>3.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1_201</td>\n",
              "      <td>I will agree at some what extend, because if w...</td>\n",
              "      <td>2.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>5</td>\n",
              "      <td>5_146</td>\n",
              "      <td>Earth is a creation of God and everything that...</td>\n",
              "      <td>2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>5</td>\n",
              "      <td>5_65</td>\n",
              "      <td>production of arms and weapons in this present...</td>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>5</td>\n",
              "      <td>5_151</td>\n",
              "      <td>Race to become more powerful can destroy the e...</td>\n",
              "      <td>2.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>5</td>\n",
              "      <td>5_404</td>\n",
              "      <td>In its attempt to harness the power of the ato...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>5</td>\n",
              "      <td>5_360</td>\n",
              "      <td>Racein the production of arms and weapons in t...</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>305 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     promptId  ... predicted_score\n",
              "0           1  ...             3.0\n",
              "1           1  ...             3.2\n",
              "2           1  ...             2.5\n",
              "3           1  ...             3.2\n",
              "4           1  ...             2.7\n",
              "..        ...  ...             ...\n",
              "300         5  ...             2.2\n",
              "301         5  ...             1.1\n",
              "302         5  ...             2.7\n",
              "303         5  ...             3.0\n",
              "304         5  ...             1.5\n",
              "\n",
              "[305 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM_th2-JUFpv"
      },
      "source": [
        "Writing the data with the predictions in the csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDF47bOwFqwa"
      },
      "source": [
        "fileVariable = open('test.csv', 'r+')\n",
        "fileVariable.truncate(0)\n",
        "fileVariable.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGZh6mRETfvJ"
      },
      "source": [
        "test_dataset.to_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXUBBS4RT2-i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}